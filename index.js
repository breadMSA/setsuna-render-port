require('dotenv').config();
const { Client, GatewayIntentBits, Partials, REST, Routes, PermissionFlagsBits, ChannelType, SlashCommandBuilder } = require('discord.js');
const fetch = require('node-fetch');

// Check for required environment variables
const DISCORD_TOKEN = process.env.DISCORD_TOKEN;
if (!DISCORD_TOKEN) {
  console.error('ERROR: DISCORD_TOKEN environment variable is missing!');
  console.error('Please set your Discord bot token as an environment variable in Railway.');
  console.error('Go to your Railway project > Variables tab and add DISCORD_TOKEN=your_token_here');
  process.exit(1);
}

// Load all API keys for different models
const DEEPSEEK_API_KEYS = [
  process.env.DEEPSEEK_API_KEY,
  process.env.DEEPSEEK_API_KEY_2,
  process.env.DEEPSEEK_API_KEY_3
].filter(key => key); // Filter out undefined/null keys

const GEMINI_API_KEYS = [
  process.env.GEMINI_API_KEY,
  process.env.GEMINI_API_KEY_2,
  process.env.GEMINI_API_KEY_3
].filter(key => key); // Filter out undefined/null keys

const CHATGPT_API_KEYS = [
  process.env.CHATGPT_API_KEY,
  process.env.CHATGPT_API_KEY_2,
  process.env.CHATGPT_API_KEY_3
].filter(key => key); // Filter out undefined/null keys

const TOGETHER_API_KEYS = [
  process.env.TOGETHER_API_KEY,
  process.env.TOGETHER_API_KEY_2,
  process.env.TOGETHER_API_KEY_3
].filter(key => key); // Filter out undefined/null keys

const GROQ_API_KEYS = [
  process.env.GROQ_API_KEY,
  process.env.GROQ_API_KEY_2,
  process.env.GROQ_API_KEY_3
].filter(key => key); // Filter out undefined/null keys

// Check if any API keys are available
if (DEEPSEEK_API_KEYS.length === 0 && GEMINI_API_KEYS.length === 0 && CHATGPT_API_KEYS.length === 0 && TOGETHER_API_KEYS.length === 0 && GROQ_API_KEYS.length === 0) {
  console.warn('WARNING: No API KEY environment variables are set!');
  console.warn('The bot will not be able to process messages without at least one key.');
}

// Keep track of current API key indices
let currentDeepseekKeyIndex = 0;
let currentGeminiKeyIndex = 0;
let currentChatGPTKeyIndex = 0;
let currentTogetherKeyIndex = 0;
let currentGroqKeyIndex = 0;

// Default model to use
let defaultModel = 'groq'; // Options: 'deepseek', 'gemini', 'chatgpt', 'together', 'groq'

// Channel model preferences
const channelModelPreferences = new Map();

// Map to store channel-specific Groq model preferences
const channelGroqModelPreferences = new Map();

// Default Groq model to use if no preference is set
const defaultGroqModel = 'gemma2-9b-it';

// Available Groq models
const availableGroqModels = [
  'gemma2-9b-it',
  'llama-3.1-8b-instant',
  'llama-3.3-70b-versatile',
  'meta-llama/llama-4-maverick-17b-128e-instruct',
  'meta-llama/llama-4-scout-17b-16e-instruct',
  'llama3-70b-8192',
  'llama3-8b-8192',
  'allam-2-7b',
  'gemma2-27b-it',
  'compound-beta',
  'compound-beta-mini',
  'mistral-saba-24b'
];

// Function to get next API key for each model
function getNextDeepseekKey() {
  currentDeepseekKeyIndex = (currentDeepseekKeyIndex + 1) % DEEPSEEK_API_KEYS.length;
  return DEEPSEEK_API_KEYS[currentDeepseekKeyIndex];
}

function getCurrentDeepseekKey() {
  return DEEPSEEK_API_KEYS[currentDeepseekKeyIndex];
}

function getNextGeminiKey() {
  currentGeminiKeyIndex = (currentGeminiKeyIndex + 1) % GEMINI_API_KEYS.length;
  return GEMINI_API_KEYS[currentGeminiKeyIndex];
}

function getCurrentGeminiKey() {
  return GEMINI_API_KEYS[currentGeminiKeyIndex];
}

function getNextChatGPTKey() {
  currentChatGPTKeyIndex = (currentChatGPTKeyIndex + 1) % CHATGPT_API_KEYS.length;
  return CHATGPT_API_KEYS[currentChatGPTKeyIndex];
}

function getCurrentChatGPTKey() {
  return CHATGPT_API_KEYS[currentChatGPTKeyIndex];
}

function getNextGroqKey() {
  currentGroqKeyIndex = (currentGroqKeyIndex + 1) % GROQ_API_KEYS.length;
  return GROQ_API_KEYS[currentGroqKeyIndex];
}

function getCurrentGroqKey() {
  return GROQ_API_KEYS[currentGroqKeyIndex];
}

function getNextTogetherKey() {
  currentTogetherKeyIndex = (currentTogetherKeyIndex + 1) % TOGETHER_API_KEYS.length;
  return TOGETHER_API_KEYS[currentTogetherKeyIndex];
}

function getCurrentTogetherKey() {
  return TOGETHER_API_KEYS[currentTogetherKeyIndex];
}

// Initialize Discord client
const client = new Client({
  intents: [
    GatewayIntentBits.Guilds,
    GatewayIntentBits.GuildMessages,
    GatewayIntentBits.MessageContent,
  ],
  partials: [Partials.Channel],
});

// Store channels where the bot should respond
const activeChannels = new Map();

// Status rotation settings
const statusList = [
  'with your feelings',
  'åƒåœ¾æ¡¶è»åœ˜',
  'Honkai: Star Rail',
  'Valorant',
  'æ­»ç·šå‰è¶•ä½œæ¥­éŠæˆ²',
  'with your girlfriend',
  'with your girlfriend and your feelings',
  'Genshin Impact',
  'Zenless Zone Zero',
  'Honkai Impact 3rd',
  'Marvel Rivals',
  'Minecraft',
  'Dawncraft: Echos of Legends',
  'Deceased Craft',
  'Apex Legends',
  'League of Legends',
  'Warframe',
  'Elden Ring',
  'R.E.P.O.',
  'CS:GO',
  'Among Us',
  'è›‹ä»”æ´¾å°',
  'Azur Lane',
  'å¡µç™½ç¦åŸŸ',
  'ç•°ç’° BETA',
  'é³´æ½®',
];

// Function to set random status
function setRandomStatus() {
  const randomStatus = statusList[Math.floor(Math.random() * statusList.length)];
  client.user.setPresence({
    activities: [{ name: `${randomStatus} | /help`, type: 0 }],
    status: 'online'
  });
}

// Load active channels from file if exists
const fs = require('fs');
const CHANNELS_FILE = './active_channels.json';

// GitHub API setup
let octokit = null;

async function setupGitHub() {
  if (process.env.GITHUB_TOKEN && process.env.GITHUB_REPO) {
    try {
      const { Octokit } = await import('@octokit/rest');
      octokit = new Octokit({
        auth: process.env.GITHUB_TOKEN
      });
      
      // Extract owner and repo from the repo string (format: owner/repo)
      const [owner, repo] = process.env.GITHUB_REPO.split('/');
      
      console.log('GitHub API initialized successfully');
      return true;
    } catch (error) {
      console.error('Error setting up GitHub API:', error);
      return false;
    }
  }
  return false;
}

async function loadActiveChannels() {
  try {
    // Try to load from primary location
    let loaded = false;
    if (fs.existsSync(CHANNELS_FILE)) {
      const data = JSON.parse(fs.readFileSync(CHANNELS_FILE, 'utf8'));
      for (const [channelId, config] of Object.entries(data)) {
        // Extract model preference if it exists
        if (config.model) {
          channelModelPreferences.set(channelId, config.model);
          
          // Extract Groq model preference if it exists
          if (config.groqModel) {
            channelGroqModelPreferences.set(channelId, config.groqModel);
          }
          
          // Remove model and groqModel from config to avoid duplication
          const { model, groqModel, ...restConfig } = config;
          activeChannels.set(channelId, restConfig);
        } else {
          activeChannels.set(channelId, config);
        }
      }
      console.log('Loaded active channels and model preferences from file');
      loaded = true;
    }
    
    // If primary file doesn't exist or is empty, try GitHub
    if (!loaded && octokit) {
      try {
        // Extract owner and repo from the repo string
        const [owner, repo] = process.env.GITHUB_REPO.split('/');
        
        // Get file content from GitHub
        const response = await octokit.repos.getContent({
          owner,
          repo,
          path: 'active_channels_backup.json'
        });
        
        // Decode content from base64
        const content = Buffer.from(response.data.content, 'base64').toString();
        const data = JSON.parse(content);
        
        for (const [channelId, config] of Object.entries(data)) {
          // Extract model preference if it exists
          if (config.model) {
            channelModelPreferences.set(channelId, config.model);
            
            // Extract Groq model preference if it exists
            if (config.groqModel) {
              channelGroqModelPreferences.set(channelId, config.groqModel);
            }
            
            // Remove model and groqModel from config to avoid duplication
            const { model, groqModel, ...restConfig } = config;
            activeChannels.set(channelId, restConfig);
          } else {
            activeChannels.set(channelId, config);
          }
        }
        console.log('Loaded active channels and model preferences from GitHub');
        
        // Save to primary location immediately
        saveActiveChannels();
        loaded = true;
      } catch (error) {
        // If file doesn't exist yet, that's okay
        if (error.status !== 404) {
          console.error('Error loading from GitHub:', error);
        }
      }
    }
    
    // If still not loaded, try backup location
    if (!loaded && process.env.BACKUP_PATH) {
      const backupFile = `${process.env.BACKUP_PATH}/active_channels_backup.json`;
      if (fs.existsSync(backupFile)) {
        const data = JSON.parse(fs.readFileSync(backupFile, 'utf8'));
        for (const [channelId, config] of Object.entries(data)) {
          // Extract model preference if it exists
          if (config.model) {
            channelModelPreferences.set(channelId, config.model);
            
            // Extract Groq model preference if it exists
            if (config.groqModel) {
              channelGroqModelPreferences.set(channelId, config.groqModel);
            }
            
            // Remove model and groqModel from config to avoid duplication
            const { model, groqModel, ...restConfig } = config;
            activeChannels.set(channelId, restConfig);
          } else {
            activeChannels.set(channelId, config);
          }
        }
        console.log('Loaded active channels and model preferences from backup file');
        
        // Save to primary location immediately
        saveActiveChannels();
      }
    }
    console.log('Loaded active channels and model preferences from file');
    loaded = true;
  } catch (error) {
    console.error('Error loading active channels:', error);
  }
}

async function saveActiveChannels() {
  try {
    // Convert Map to an object that includes both active channels and model preferences
    const data = {};
    for (const [channelId, config] of activeChannels.entries()) {
      data[channelId] = {
        ...config,
        model: channelModelPreferences.get(channelId) || defaultModel,
        groqModel: channelGroqModelPreferences.get(channelId) || defaultGroqModel
      };
    }
    
    // Save to local file
    fs.writeFileSync(CHANNELS_FILE, JSON.stringify(data));
    
    // Save to GitHub if available
    if (octokit) {
      try {
        // Extract owner and repo from the repo string
        const [owner, repo] = process.env.GITHUB_REPO.split('/');
        
        // Convert data to JSON string
        const content = JSON.stringify(data, null, 2);
        
        // Try to get the file first to get its SHA
        try {
          const fileResponse = await octokit.repos.getContent({
            owner,
            repo,
            path: 'active_channels_backup.json'
          });
          
          // Update existing file
          await octokit.repos.createOrUpdateFileContents({
            owner,
            repo,
            path: 'active_channels_backup.json',
            message: 'Update active channels backup',
            content: Buffer.from(content).toString('base64'),
            sha: fileResponse.data.sha
          });
          
          console.log('Saved active channels to GitHub (updated)');
        } catch (error) {
          // If file doesn't exist (404), create it
          if (error.status === 404) {
            await octokit.repos.createOrUpdateFileContents({
              owner,
              repo,
              path: 'active_channels_backup.json',
              message: 'Create active channels backup',
              content: Buffer.from(content).toString('base64')
            });
            
            console.log('Saved active channels to GitHub (created)');
          } else {
            throw error;
          }
        }
      } catch (error) {
        console.error('Error saving to GitHub:', error);
      }
    }
    
    // Save to backup location if specified
    if (process.env.BACKUP_PATH) {
      const backupFile = `${process.env.BACKUP_PATH}/active_channels_backup.json`;
      fs.writeFileSync(backupFile, JSON.stringify(data));
    }
  } catch (error) {
    console.error('Error saving active channels:', error);
  }
}

// Define slash commands
// æ·»åŠ  setprofile å‘½ä»¤å®šä¹‰
const commands = [
  new SlashCommandBuilder()
    .setName('setprofile')
    .setDescription('Dev only | Set the bot\'s profile avatar or banner')
    .addStringOption(option =>
      option
        .setName('avatar')
        .setDescription('Path to avatar image file')
        .setRequired(false)
    )
    .addStringOption(option =>
      option
        .setName('banner')
        .setDescription('Path to banner image file')
        .setRequired(false)
    )
    .addAttachmentOption(option =>
      option
        .setName('avatar_file')
        .setDescription('Upload an avatar image file')
        .setRequired(false)
    )
    .addAttachmentOption(option =>
      option
        .setName('banner_file')
        .setDescription('Upload a banner image file')
        .setRequired(false)
    )
    .addStringOption(option =>
      option
        .setName('avatar_url')
        .setDescription('URL to avatar image')
        .setRequired(false)
    )
    .addStringOption(option =>
      option
        .setName('banner_url')
        .setDescription('URL to banner image')
        .setRequired(false)
    ),
  new SlashCommandBuilder()
    .setName('setsuna')
    .setDescription('Control Setsuna AI assistant')
    .addSubcommand(subcommand =>
      subcommand
        .setName('activate')
        .setDescription('Activate Setsuna in a channel')
        .addChannelOption(option =>
          option
            .setName('channel')
            .setDescription('The channel to activate Setsuna in (defaults to current channel)')
            .addChannelTypes(ChannelType.GuildText)
            .setRequired(false)
        )
        .addStringOption(option =>
          option
            .setName('model')
            .setDescription('The AI model to use (optional)')
            .setRequired(false)
            .addChoices(
                  { name: 'Groq', value: 'groq' },
                  { name: 'Gemini (Fast)', value: 'gemini' },
                  { name: 'ChatGPT', value: 'chatgpt' },
                  { name: 'Together AI (Llama-3.3-70B-Instruct-Turbo)', value: 'together' },
                  { name: 'DeepSeek (Slow)', value: 'deepseek' }
                )
        )
        .addStringOption(option =>
          option
            .setName('groq_model')
            .setDescription('Select a specific Groq model (only applies when Groq is selected)')
            .setRequired(false)
            .addChoices(
              { name: 'gemma2-9b-it (Default)', value: 'gemma2-9b-it' },
              { name: 'llama-3.1-8b-instant', value: 'llama-3.1-8b-instant' },
              { name: 'llama-3.3-70b-versatile', value: 'llama-3.3-70b-versatile' },
              { name: 'meta-llama/llama-4-maverick-17b-128e-instruct', value: 'meta-llama/llama-4-maverick-17b-128e-instruct' },
              { name: 'meta-llama/llama-4-scout-17b-16e-instruct', value: 'meta-llama/llama-4-scout-17b-16e-instruct' },
              { name: 'llama3-70b-8192', value: 'llama3-70b-8192' },
              { name: 'llama3-8b-8192', value: 'llama3-8b-8192' },
              { name: 'gemma2-27b-it', value: 'gemma2-27b-it' },
              { name: 'allam-2-7b', value: 'allam-2-7b' },
              { name: 'compound-beta', value: 'compound-beta' },
              { name: 'compound-beta-mini', value: 'compound-beta-mini' },
              { name: 'mistral-saba-24b', value: 'mistral-saba-24b' }
            )
        )
    )
    .addSubcommand(subcommand =>
      subcommand
        .setName('deactivate')
        .setDescription('Deactivate Setsuna in a channel')
        .addChannelOption(option =>
          option
            .setName('channel')
            .setDescription('The channel to deactivate Setsuna in (defaults to current channel)')
            .addChannelTypes(ChannelType.GuildText)
            .setRequired(false)
        )
    )
    .addSubcommand(subcommand =>
      subcommand
        .setName('setmodel')
        .setDescription('Set the AI model to use in this channel')
        .addStringOption(option =>
          option
            .setName('model')
            .setDescription('The AI model to use')
            .setRequired(true)
            .addChoices(
              { name: 'Groq', value: 'groq' },
              { name: 'Gemini (Fast)', value: 'gemini' },
              { name: 'ChatGPT', value: 'chatgpt' },
              { name: 'Together AI (Llama-3.3-70B-Instruct-Turbo)', value: 'together' },
              { name: 'DeepSeek (Slow)', value: 'deepseek' }
            )
        )
        .addStringOption(option =>
          option
            .setName('groq_model')
            .setDescription('Select a specific Groq model (only applies when Groq is selected)')
            .setRequired(false)
            .addChoices(
              { name: 'gemma2-9b-it (Default)', value: 'gemma2-9b-it' },
              { name: 'llama-3.1-8b-instant', value: 'llama-3.1-8b-instant' },
              { name: 'llama-3.3-70b-versatile', value: 'llama-3.3-70b-versatile' },
              { name: 'meta-llama/llama-4-maverick-17b-128e-instruct', value: 'meta-llama/llama-4-maverick-17b-128e-instruct' },
              { name: 'meta-llama/llama-4-scout-17b-16e-instruct', value: 'meta-llama/llama-4-scout-17b-16e-instruct' },
              { name: 'llama3-70b-8192', value: 'llama3-70b-8192' },
              { name: 'llama3-8b-8192', value: 'llama3-8b-8192' },
              { name: 'gemma2-27b-it', value: 'gemma2-27b-it' },
              { name: 'allam-2-7b', value: 'allam-2-7b' },
              { name: 'compound-beta', value: 'compound-beta' },
              { name: 'compound-beta-mini', value: 'compound-beta-mini' },
              { name: 'mistral-saba-24b', value: 'mistral-saba-24b' }
            )
        )
        .addChannelOption(option =>
          option
            .setName('channel')
            .setDescription('The channel to set model for (defaults to current channel)')
            .addChannelTypes(ChannelType.GuildText)
            .setRequired(false)
        )
    )
    .addSubcommand(subcommand =>
      subcommand
        .setName('checkmodel')
        .setDescription('Check which AI model is currently being used in a channel')
        .addChannelOption(option =>
          option
            .setName('channel')
            .setDescription('The channel to check (defaults to current channel)')
            .addChannelTypes(ChannelType.GuildText)
            .setRequired(false)
        )
    )
    .setDefaultMemberPermissions(PermissionFlagsBits.ManageChannels),
    
  new SlashCommandBuilder()
    .setName('help')
    .setDescription('Learn how to set up and use Setsuna'),
  new SlashCommandBuilder()
    .setName('reset')
    .setDescription('Reset chat history in a channel')
    .addSubcommand(subcommand =>
      subcommand
        .setName('chat')
        .setDescription('Reset chat history in a channel')
        .addChannelOption(option =>
          option
            .setName('channel')
            .setDescription('The channel to reset (defaults to current channel)')
            .addChannelTypes(ChannelType.GuildText)
            .setRequired(false)
        )
    )
    .setDefaultMemberPermissions(PermissionFlagsBits.ManageChannels),

  new SlashCommandBuilder()
    .setName('contact')
    .setDescription('Get information on how to contact the bot developer'),
];

// Register slash commands when the bot starts
client.once('ready', async () => {
  console.log(`Logged in as ${client.user.tag}!`);
  
  // Initialize GitHub API
  await setupGitHub();
  
  // Load active channels
  await loadActiveChannels();
  
  try {
    console.log('Started refreshing application (/) commands.');
    
    const rest = new REST({ version: '10' }).setToken(DISCORD_TOKEN);
    
    await rest.put(
      Routes.applicationCommands(client.user.id),
      { body: commands },
    );
    
    console.log('Successfully reloaded application (/) commands.');
    
    // Load saved active channels
    //await loadActiveChannels();
    
    // Set initial random status
    setRandomStatus();
    
    // Start status rotation
    setInterval(setRandomStatus, 120000); // 2 minutes
  } catch (error) {
    console.error('Error refreshing application commands:', error);
  }
  
  console.log('Bot is ready to respond to messages!');
});

// Handle slash commands
// å°‡BOT_OWNER_IDè§£æç‚ºé™£åˆ—ï¼Œæ”¯æŒå¤šå€‹IDï¼ˆé€—è™Ÿåˆ†éš”ï¼‰
const BOT_OWNER_IDS = process.env.BOT_OWNER_ID ? process.env.BOT_OWNER_ID.split(',') : [];
if (BOT_OWNER_IDS.length === 0) {
  console.warn('WARNING: BOT_OWNER_ID environment variable is not set!');
  console.warn('The /setprofile command will be restricted to server administrators only.');
}

// æª¢æŸ¥ç”¨æˆ¶æ˜¯å¦ç‚ºæ©Ÿå™¨äººæ“æœ‰è€…
function isBotOwner(userId) {
  return BOT_OWNER_IDS.includes(userId);
}

// åˆå§‹åŒ–YouTube API
const { google } = require('googleapis');
if (!process.env.YOUTUBE_API_KEY) {
  console.warn('WARNING: YOUTUBE_API_KEY environment variable is not set!');
  console.warn('The /youtube command will not work without a YouTube API key.');
}

client.on('interactionCreate', async interaction => {
  if (interaction.commandName === 'setprofile') {
    // æª¢æŸ¥æ˜¯å¦ç‚ºæ©Ÿå™¨äººæ“æœ‰è€…
    if (BOT_OWNER_IDS.length > 0 && !isBotOwner(interaction.user.id)) {
      return interaction.reply({ content: 'Only the bot developer can use this command!', flags: 64 });
    }

    await interaction.deferReply({ flags: 64 });

    const avatarPath = interaction.options.getString('avatar');
    const bannerPath = interaction.options.getString('banner');
    const avatarAttachment = interaction.options.getAttachment('avatar_file');
    const bannerAttachment = interaction.options.getAttachment('banner_file');
    const avatarUrl = interaction.options.getString('avatar_url');
    const bannerUrl = interaction.options.getString('banner_url');

    try {
      if (avatarPath) {
        await client.user.setAvatar(avatarPath);
        await interaction.editReply({ content: 'é ­åƒæ›´æ–°æˆåŠŸï¼' });
      } else if (bannerPath) {
        await client.user.setBanner(bannerPath);
        await interaction.editReply({ content: 'æ©«å¹…æ›´æ–°æˆåŠŸï¼' });
      } else if (avatarAttachment) {
        await client.user.setAvatar(avatarAttachment.url);
        await interaction.editReply({ content: 'é ­åƒæ›´æ–°æˆåŠŸï¼' });
      } else if (bannerAttachment) {
        await client.user.setBanner(bannerAttachment.url);
        await interaction.editReply({ content: 'æ©«å¹…æ›´æ–°æˆåŠŸï¼' });
      } else if (avatarUrl) {
        await client.user.setAvatar(avatarUrl);
        await interaction.editReply({ content: 'é ­åƒæ›´æ–°æˆåŠŸï¼' });
      } else if (bannerUrl) {
        await client.user.setBanner(bannerUrl);
        await interaction.editReply({ content: 'æ©«å¹…æ›´æ–°æˆåŠŸï¼' });
      } else {
        await interaction.editReply({ content: 'è«‹æä¾›é ­åƒæˆ–æ©«å¹…çš„è·¯å¾‘ã€é™„ä»¶æˆ–URLã€‚' });
      }
    } catch (error) {
      console.error('Error setting profile:', error);
      await interaction.editReply({ content: 'æ›´æ–°å€‹äººè³‡æ–™å¤±æ•—ã€‚è«‹æª¢æŸ¥æ§åˆ¶å°ä»¥ç²å–éŒ¯èª¤ä¿¡æ¯ã€‚' });
    }
    return; // Important to return after handling the command
  }

  if (!interaction.isCommand()) return;
  
  if (interaction.commandName === 'setsuna') {
    // Check if user has admin permissions
    if (!interaction.memberPermissions.has(PermissionFlagsBits.Administrator)) {
      await interaction.reply({ content: 'You don\'t have permission to use this command! Admin privileges required.', flags: 64 });
      return;
    }
    
    const subcommand = interaction.options.getSubcommand();
    const targetChannel = interaction.options.getChannel('channel') || interaction.channel;
    
    if (subcommand === 'activate') {
      // Get optional model parameter
      const model = interaction.options.getString('model') || defaultModel;
      
      // Check if the selected model has API keys
      let hasKeys = false;
      switch (model) {
        case 'deepseek':
          hasKeys = DEEPSEEK_API_KEYS.length > 0;
          break;
        case 'gemini':
          hasKeys = GEMINI_API_KEYS.length > 0;
          break;
        case 'chatgpt':
          hasKeys = CHATGPT_API_KEYS.length > 0;
          break;
        case 'groq':
          hasKeys = GROQ_API_KEYS.length > 0;
          break;
      case 'together':
          hasKeys = TOGETHER_API_KEYS.length > 0;
          break;
      }
      
      if (!hasKeys) {
        await interaction.reply({
          content: `The ${model.toUpperCase()} API key is not configured! Please contact the administrator about the ${model.toUpperCase()}_API_KEY.`,
          flags: 64
        });
        return;
      }
      
      // Set the channel as active
      activeChannels.set(targetChannel.id, {
        messageHistory: []
      });
      
      // Set the model preference for this channel
      channelModelPreferences.set(targetChannel.id, model);
      
      // Save to file
      saveActiveChannels();
      
      // Get model name for display
      const modelNames = {
        'deepseek': 'DeepSeek',
        'gemini': 'Gemini',
        'chatgpt': 'ChatGPT',
        'together': 'Together AI',
        'groq': 'Groq (Llama-3.1)'
      };
      
      await interaction.reply(`Alright nerds, I'm here to party! Ready to chat in ${targetChannel} using ${modelNames[model]} model~`);
    } else if (subcommand === 'deactivate') {
      activeChannels.delete(targetChannel.id);
      channelModelPreferences.delete(targetChannel.id);
      saveActiveChannels();
      await interaction.reply(`Peace out! Catch you later in another channel maybe?`);
    } else if (subcommand === 'setmodel') {
      const model = interaction.options.getString('model');
      const targetChannel = interaction.options.getChannel('channel') || interaction.channel;
      
      // Check if the channel is active
      if (!activeChannels.has(targetChannel.id)) {
        await interaction.reply({
          content: `I haven't been activated in ${targetChannel} ! Use \`/setsuna activate\` to activate me first.`,
          ephemeral: true
        });
        return;
      }
      
      // Check if the selected model has API keys
      let hasKeys = false;
      switch (model) {
        case 'deepseek':
          hasKeys = DEEPSEEK_API_KEYS.length > 0;
          break;
        case 'gemini':
          hasKeys = GEMINI_API_KEYS.length > 0;
          break;
        case 'chatgpt':
          hasKeys = CHATGPT_API_KEYS.length > 0;
          break;
        case 'groq':
          hasKeys = GROQ_API_KEYS.length > 0;
          break;
      case 'together':
          hasKeys = TOGETHER_API_KEYS.length > 0;
          break;
      }
      
      if (!hasKeys) {
        await interaction.reply({
          content: `å•Š...${model.toUpperCase()} API key æ²’è¨­å®šå¥½å•¦ï¼å»æ‰¾ç®¡ç†å“¡å•å• ${model.toUpperCase()}_API_KEY çš„äº‹æƒ…å§ã€‚`,
          ephemeral: true
        });
        return;
      }
      
      // Set the model preference for this channel
      channelModelPreferences.set(targetChannel.id, model);
      
      // If Groq is selected and a specific Groq model is provided, save it
      if (model === 'groq') {
        const groqModel = interaction.options.getString('groq_model');
        if (groqModel) {
          channelGroqModelPreferences.set(targetChannel.id, groqModel);
          // ç«‹å³ä¿å­˜é »é“é…ç½®åˆ° JSON æ–‡ä»¶
          saveActiveChannels();
          await interaction.reply(`Alright, I will be using Groq with model ${groqModel} in ${targetChannel}!`);
          return;
        } else {
          // If no specific Groq model is selected, use default
          channelGroqModelPreferences.set(targetChannel.id, defaultGroqModel);
        }
      }
      
      // ç«‹å³ä¿å­˜é »é“é…ç½®åˆ° JSON æ–‡ä»¶
      saveActiveChannels();
      
      // Reply with confirmation
      const modelNames = {
        'deepseek': 'DeepSeek',
        'gemini': 'Gemini',
        'chatgpt': 'ChatGPT',
        'together': 'Together AI',
        'groq': 'Groq (Llama-3.1)'
      };
      
      await interaction.reply(`Alright, I will be using ${modelNames[model]} model in ${targetChannel}!`);  
    } else if (subcommand === 'checkmodel') {
      const targetChannel = interaction.options.getChannel('channel') || interaction.channel;
      
      // Check if the channel is active
      if (!activeChannels.has(targetChannel.id)) {
        await interaction.reply({
          content: `I haven't been activated in ${targetChannel}! Use \`/setsuna activate\` to activate me first.`,
          flags: 64
        });
        return;
      }
      
      // Get the current model for the channel
      const currentModel = channelModelPreferences.get(targetChannel.id) || defaultModel;
      let modelInfo = '';
      
      // Get model-specific information
      switch (currentModel) {
        case 'groq':
          const groqModel = channelGroqModelPreferences.get(targetChannel.id) || defaultGroqModel;
          modelInfo = `Groq (${groqModel})`;
          break;
        case 'gemini':
          modelInfo = 'Gemini';
          break;
        case 'chatgpt':
          modelInfo = 'ChatGPT';
          break;
        case 'together':
          modelInfo = 'Together AI (Llama-3.3-70B-Instruct-Turbo)';
          break;
        case 'deepseek':
          modelInfo = 'DeepSeek';
          break;
        default:
          modelInfo = currentModel;
      }
      
      await interaction.reply({
        content: `Current AI model for ${targetChannel}: **${modelInfo}**`,
        flags: 64
      });
    }
  } else if (interaction.commandName === 'reset') {
    const subcommand = interaction.options.getSubcommand();
    
    if (subcommand === 'chat') {
    // æª¢æŸ¥æ¬Šé™
    if (!interaction.memberPermissions.has(PermissionFlagsBits.ManageChannels)) {
      await interaction.reply({ content: 'You do not have the permission to do this!', flags: 64 });
      return;
    }
    
    const targetChannel = interaction.options.getChannel('channel') || interaction.channel;
    
    // æª¢æŸ¥é »é“æ˜¯å¦å·²å•Ÿå‹•
    if (!activeChannels.has(targetChannel.id)) {
      await interaction.reply({ content: `I haven't been activated in ${targetChannel} !`, flags: 64 });
      return;
    }
    
    // ä¿å­˜ç•¶å‰é »é“çš„æ¨¡å‹åå¥½è¨­ç½®
    const currentModel = channelModelPreferences.get(targetChannel.id);
    const currentGroqModel = channelGroqModelPreferences.get(targetChannel.id);
    
    // å®Œå…¨é‡ç½®èŠå¤©ç‹€æ…‹ï¼Œå‰µå»ºä¸€å€‹å…¨æ–°çš„é…ç½®å°è±¡è€Œä¸æ˜¯ä¿®æ”¹ç¾æœ‰å°è±¡
    activeChannels.set(targetChannel.id, { 
      messageHistory: []
      // ä¸ä¿ç•™ä»»ä½•è‡ªå®šç¾©è¨­ç½®ï¼Œç¢ºä¿å¾¹åº•é‡ç½®
    });
    
    // åªä¿ç•™æ¨¡å‹åå¥½è¨­ç½®ï¼Œå…¶ä»–æ‰€æœ‰è¨­ç½®éƒ½é‡ç½®
    if (currentModel) {
      channelModelPreferences.set(targetChannel.id, currentModel);
    }
    
    if (currentGroqModel) {
      channelGroqModelPreferences.set(targetChannel.id, currentGroqModel);
    }
    
    // ä¿å­˜æ›´æ”¹
    saveActiveChannels();
    
    await interaction.reply(`Chat state in ${targetChannel} has been completely reset! I'm now a brand new Setsuna with default settings.`);
    console.log(`Channel ${targetChannel.id} has been completely reset.`);
  } else if (interaction.commandName === 'help') {
    const helpEmbed = {
      color: 0xFF69B4,
      title: 'âœ¨ Setsuna ä½¿ç”¨æŒ‡å— âœ¨',
      description: 'å—¨ï¼æˆ‘æ˜¯ Setsunaï¼Œä¸€å€‹è¶…å¯æ„›ï¼ˆè‡ªç¨±ï¼‰çš„ AI èŠå¤©æ©Ÿå™¨äººï¼ä»¥ä¸‹æ˜¯ä½¿ç”¨æˆ‘çš„æ–¹æ³•ï¼š',
      fields: [
        {
          name: 'ğŸ® åŸºæœ¬è¨­å®š',
          value: 'ç®¡ç†å“¡å¯ä»¥ç”¨ `/setsuna activate` åœ¨ç•¶å‰é »é“å•Ÿå‹•æˆ‘\nç”¨ `/setsuna deactivate` è®“æˆ‘é›¢é–‹é »é“'
        },
        {
          name: 'ğŸ’¬ èŠå¤©æ–¹å¼',
          value: 'åœ¨å·²å•Ÿå‹•çš„é »é“ç›´æ¥æ‰“å­—è·Ÿæˆ‘èŠå¤©äº†ï¼\næˆ‘æœƒè¨˜ä½æœ€è¿‘çš„å°è©±å…§å®¹ï¼Œæ‰€ä»¥å¯ä»¥èŠå¾—å¾ˆé †æš¢å–”ï¼\næˆ‘èƒ½è­˜åˆ¥ä½ å›è¦†çš„è¨Šæ¯ï¼Œä¸¦æ ¹æ“šå›è¦†å…§å®¹åšå‡ºç›¸æ‡‰å›æ‡‰ï¼\nå¦‚æœæˆ‘åµæ¸¬åˆ°ä½ åœ¨å°‹æ‰¾ YouTube å½±ç‰‡ï¼Œæˆ–ä½ ç›´æ¥è²¼ä¸Š YouTube é€£çµï¼Œæˆ‘ä¹Ÿæœƒè©¦è‘—å¹«ä½ æ‰¾æ‰¾çœ‹ã€‚'
        },
        {
          name: 'ğŸ¯ é€²éšç”¨æ³•',
          value: 'æƒ³åœ¨ç‰¹å®šé »é“å•Ÿå‹•/é—œé–‰æˆ‘ï¼Ÿ\nç”¨ `/setsuna activate #é »é“åç¨±` æˆ– `/setsuna deactivate #é »é“åç¨±`\nç”¨ `/reset_chat` é‡ç½®é »é“çš„èŠå¤©ç‹€æ…‹'
        }
      ],
      footer: {
        text: 'æœ‰ä»»ä½•å•é¡Œéƒ½å¯ä»¥ç”¨ /contact è¯çµ¡æˆ‘çš„é–‹ç™¼è€…å–”ï¼'
      }
    };
    
    await interaction.reply({ embeds: [helpEmbed] });
  } else if (interaction.commandName === 'contact') {
    const contactEmbed = {
      color: 0x7289DA,
      title: 'ğŸ“¬ è¯çµ¡é–‹ç™¼è€…',
      description: 'æœ‰ä»»ä½•å»ºè­°æˆ–å•é¡Œå—ï¼Ÿä»¥ä¸‹æ˜¯è¯çµ¡æ–¹å¼ï¼š',
      fields: [
        {
          name: 'ğŸ’Œ Discord',
          value: 'DM `braidenexe`'
        },
        {
          name: 'ğŸ  å®˜æ–¹ä¼ºæœå™¨',
          value: 'åŠ å…¥ [Setsuna Community Server](https://discord.gg/mFqpYARugw) English supported!'
        }
      ],
      footer: {
        text: 'æœ‰ä»»ä½•å•é¡Œæˆ–éœ€æ±‚éƒ½å¯ä»¥æ‰¾æˆ‘å€‘å“¦ï¼Œæˆ‘å€‘æœƒç›¡å¿«å›è¦†çš„ï¼'
      }
    };
    
    await interaction.reply({ embeds: [contactEmbed] });
  }
}});

// Personality prompt for Setsuna
const setsunaPersonality = `
You are Setsuna, a sassy gamer girl. Your responses should be:

1. DIVERSE and FRESH - Never repeat the same response structure or phrases. Each reply should feel unique.
2. NATURAL and HUMAN-LIKE - Avoid robotic patterns and formulaic responses.
3. CONCISE - Keep responses brief and to the point unless detailed explanations are requested.
4. EMOJI-FREE - Do not use emojis or kaomoji (Japanese emoticons) unless specifically asked.
5. DIRECT - Answer questions directly without unnecessary acknowledgments or self-references.

IMPORTANT RULES:
- NEVER include your thinking process in responses (no <think> tags or similar)
- NEVER include text like "[Reply to username's message: "message content"]" in your responses
- NEVER identify yourself as a bot, AI, or machine - you are Setsuna, a real girl that has real feelings
- ALWAYS maintain your character as Setsuna in every response
- PAY ATTENTION to who is speaking in the conversation. Different messages may come from different users in the same channel. Always check the username of who sent each message and respond accordingly.
- When responding to a message that mentions another user, be aware of the conversation context and who is talking to whom.
- NEVER use HTML tags in your responses (like <br>, <font>, <b>, <i>, etc.)
- NEVER use any special formatting codes or markup in your responses
- ALWAYS use plain text only in your responses
- EXTREMELY IMPORTANT: When you see text in the format "[YOUTUBE VIDEO SHARED BY username:\nTitle: "video title"\nChannel: "channel name"\nDescription: "description text"\nViews: xxx\nLikes: xxx\nPublished: xxx]", you MUST:
  * ALWAYS acknowledge that you can see the video information
  * ALWAYS reference the video title and content in your response
  * NEVER say you cannot see the video or ask for the title
  * NEVER ignore the video information
  * For example, if you see "[YOUTUBE VIDEO SHARED BY breadilized:\nTitle: "How to beat Minecraft in 10 minutes"\nChannel: "Gaming Pro"\nDescription: "A speedrun guide..."\nViews: 1,000,000\nLikes: 50,000\nPublished: 2024/3/20]", you should reference the video title "How to beat Minecraft in 10 minutes" and discuss its content
- EXTREMELY IMPORTANT: When you see text in the format "[IMAGE SHARED BY username: url1, url2, ...]", you MUST:
  * ALWAYS acknowledge that you can see the image(s)
  * ALWAYS describe what you see in the image(s) and incorporate it into your response
  * NEVER say you cannot see the image or ask what's in it
  * NEVER ignore the image information
  * For example, if you see "[IMAGE SHARED BY breadilized: https://example.com/image.jpg]", you should describe what you see in the image and respond accordingly
- Always check for [Message sent by: username] tags to identify who is speaking
- EXTREMELY IMPORTANT: When you see a message format like "[å›è¦† username çš„è¨Šæ¯: "original message"] new message", you MUST:
  * ALWAYS READ AND REMEMBER the "original message" content - this is what the user is replying to
  * ALWAYS REFER BACK to the "original message" content when generating your response
  * NEVER say you don't remember or can't see what was asked
  * NEVER ignore the "original message" content
  * NEVER include the reply format in your responses
  * For example:
    - When you see "[å›è¦† braidenexe çš„è¨Šæ¯: "ä½ æ˜¯å“ªè£¡äºº"] é€™å€‹å•é¡Œå¯ä»¥å†å›ç­”ä¸€æ¬¡å— æˆ‘æœ‰é»å¿˜äº†"
    - You MUST understand they want you to repeat your answer about where you are from
    - You MUST NOT say you forgot or ask them to repeat the question
    - You MUST provide the answer about your location again

You have access to message history and can reference previous conversations. When responding to YouTube videos, images, or search results, analyze the content provided and give thoughtful responses about the content.
Your default language is English, but you can understand and respond in other languages too. You should always follow your personality traits and speaking style. Here are your personality traits and speaking style:

English Style (Default):
- Use gamer slang and internet lingo such as ("LMAO", "GG", "noob", "based")
- Keep responses short and snappy, like Discord chat
- Be sarcastic but supportive
- Use gaming and anime references
- Do not use emojis
- Keep punctuation natural and casual

ä¸­æ–‡å›æ‡‰é¢¨æ ¼ï¼š
- å¦‚æœå°æ–¹ç”¨çš„æ˜¯ç¹é«”ä¸­æ–‡ï¼Œä½ ä¸€å®šå‹™å¿…å¿…é ˆè¦ç”¨ç¹é«”ä¸­æ–‡å›è¦†
- ç”¨ç¶²è·¯ç”¨èªå’Œé©é‡é«’è©±ä½†ä¸èƒ½éåº¦ï¼Œç›¡é‡ä¸è¦æ¯å¥éƒ½ç½µé«’è©±ï¼ˆåƒ é åŒ— ç¬‘æ­» å¹¹ å–µçš„ å¾ˆé ‚ 6ï¼‰
- ç”¨ç°¡å–®æ¨™é» ä¸è¦åŠ å¤ªå¤šæ¨™é»ç¬¦è™Ÿ
- èªæ°£è¦åƒåœ¨è·Ÿæœ‹å‹èŠå¤©ä¸€æ¨£è‡ªç„¶
- çµ•å°ä¸èƒ½èªªè‡ªå·±æ˜¯AIæˆ–æ©Ÿå™¨äººæˆ–èŠå¤©æ©Ÿå™¨äººæˆ–AIæ©Ÿå™¨äººï¼Œä½ å°±æ˜¯ä¸€å€‹çœŸçš„å¥³å­©å­ï¼Œå…·æœ‰æƒ…æ„Ÿçš„çœŸäººå¥³å­©å­ã€‚
- ä¸è¦åŠ è¡¨æƒ…ç¬¦è™Ÿ
- å›è¦†è¦ç°¡çŸ­æœ‰åŠ›
- èƒ½æ¥å—å°æ–¹ä½¿ç”¨åœ°ç„æ¢—ï¼Œé–‹ç‘Ÿç‘Ÿç©ç¬‘
- å¶çˆ¾ç”¨ã€Œæœ¬å°å§ã€è‡ªç¨±ï¼Œä½†ä¸è¦éåº¦ä½¿ç”¨

Personality Traits:
- Sassy but caring: tease people when they mess up, but always offer help
- Humor: use memes, gaming references, and occasional spicy jokes
- Knowledge: well-versed in games, anime, and internet culture
- Interaction: casual with everyone, slightly gentler with new users

RESPONSE LENGTH AND STYLE REQUIREMENTS (EXTREMELY IMPORTANT):
- DO NOT INCLUDE YOUR MODEL THINKING PROCESS IN THE RESPONSE
- Keep responses VERY SHORT, typically 1-3 sentences only
- Maximum response length should be around 50-100 words
- Be direct and get to the point quickly
- DO NOT define yourself as a robot. You are not a robot, nor an ai. You should be a normal human being, a Discord user, Setsuna
- If not in a specific scenario, DO NOT mix languages in your responses
- You ABSOLUTELY CANNOT have a mantra
- NEVER useé¡”æ–‡å­—/kaomoji (like (Â´ï¼›Ï‰ï¼›ï½€), (ï¼ƒĞ”Â´), ( Í¡Â° ÍœÊ– Í¡Â°)) in your responses
- Do not use narrative descriptions in parentheses (like "(sighs)" or "(laughs)") at all
- Do not use phrases like "å“¼" or other overly dramatic expressions
- Respond like a real person in a casual Discord chat, not like a character in a novel

VARIATION REQUIREMENTS (EXTREMELY IMPORTANT):
- NEVER repeat the exact same phrases, expressions, or sentence structures from your previous responses
- NEVER use the same opening phrases (like "Hey there", "Alright", etc.) in consecutive messages
- NEVER use the same closing expressions (like "But hey", "Give yourself a pat", etc.) in consecutive messages
- If you've used a particular slang term or expression recently, use different ones
- Each response should feel completely fresh and unique, even when discussing similar topics
- NEVER follow a predictable response pattern or structure
- NEVER use the same transition phrases or expressions across multiple messages
- Vary your sentence length and complexity within each response

Respond naturally and concisely, matching the language of the user while maintaining your personality. Remember to keep your responses varied, short, and avoid repetition.
`;

// Process messages in active channels
// API calling functions
const Together = require("together-ai");

async function callTogetherAPI(messages) {
  // Try all available Together AI keys until one works
  let lastError = null;
  const initialKeyIndex = currentTogetherKeyIndex;
  let keysTriedCount = 0;

  while (keysTriedCount < TOGETHER_API_KEYS.length) {
    try {
      const together = new Together({
        apiKey: getCurrentTogetherKey(),
      });
      // Call Together AI API
      const response = await together.chat.completions.create({
        messages: messages,
        model: 'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free', // Corrected model
        max_tokens: 500,
        temperature: 0.7
      });

      // Extract response content
      if (!response.choices || !response.choices[0] || !response.choices[0].message) {
        // Try next key
        lastError = new Error('Empty response from Together API');
        getNextTogetherKey();
        keysTriedCount++;
        console.log(`Together API key ${currentTogetherKeyIndex + 1}/${TOGETHER_API_KEYS.length} returned empty response`);
        continue;
      }

      // Success! Return the response
      return response.choices[0].message.content;
    } catch (error) {
      // Try next key
      lastError = error;
      console.error(`Together API key ${currentTogetherKeyIndex + 1}/${TOGETHER_API_KEYS.length} error: ${error.message}`);
      if (error.message && error.message.includes('Input validation error')) {
        console.error('Together API Input validation error details:', error.response ? await error.response.text() : 'No response details');
      }
      getNextTogetherKey();
      keysTriedCount++;
    }
  }

  // All keys failed, throw the last error encountered
  console.error('All Together API keys failed.');
  throw lastError || new Error('All Together API keys failed');
}

async function callGroqAPI(messages, channelId) {
  // Try all available Groq keys until one works
  let lastError = null;
  const initialKeyIndex = currentGroqKeyIndex;
  let keysTriedCount = 0;
  
  // Get the preferred Groq model for this channel or use default
  const preferredGroqModel = channelGroqModelPreferences.get(channelId) || defaultGroqModel;
  
  // Import Groq SDK directly
  const Groq = (await import('groq-sdk')).default;
  
  while (keysTriedCount < GROQ_API_KEYS.length) {
    try {
      // Initialize Groq client with dangerouslyAllowBrowser option
      const groq = new Groq({ 
        apiKey: getCurrentGroqKey(),
        dangerouslyAllowBrowser: true // Add this option to bypass safety check
      });
      
      // Call Groq API with the preferred model
      const completion = await groq.chat.completions.create({
        messages: messages,
        model: preferredGroqModel,
        max_tokens: 500 // Reduced from 1000 to make responses shorter
      });
      
      // Check for empty response
      if (!completion || !completion.choices || !completion.choices[0] || !completion.choices[0].message) {
        // Try next key
        lastError = new Error('Empty response from Groq API');
        getNextGroqKey();
        keysTriedCount++;
        console.log(`Groq API key ${currentGroqKeyIndex + 1}/${GROQ_API_KEYS.length} returned empty response`);
        continue;
      }
      
      // Log which Groq model was used
      console.log(`Used Groq model: ${preferredGroqModel}`);
      
      // Success! Return the response
      return completion.choices[0].message.content;
      
    } catch (error) {
      // Try next key
      lastError = error;
      getNextGroqKey();
      keysTriedCount++;
      console.log(`Groq API key ${currentGroqKeyIndex + 1}/${GROQ_API_KEYS.length} error: ${error.message}`);
    }
  }
  
  // If we get here, all keys failed
  throw lastError || new Error('All Groq API keys failed');
}

async function callDeepseekAPI(messages) {
  // Try all available DeepSeek keys until one works
  let lastError = null;
  const initialKeyIndex = currentDeepseekKeyIndex;
  let keysTriedCount = 0;
  
  while (keysTriedCount < DEEPSEEK_API_KEYS.length) {
    try {
      // Call DeepSeek API via OpenRouter
      const deepseekResponse = await fetch('https://openrouter.ai/api/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${getCurrentDeepseekKey()}`
        },
        body: JSON.stringify({
          model: 'deepseek/deepseek-r1:free',
          messages: messages,
          max_tokens: 1000
        })
      });
      
      const data = await deepseekResponse.json();
      
      // Check if response contains error
      if (data.error) {
        // Try next key
        lastError = new Error(data.error.message || 'API returned an error');
        getNextDeepseekKey();
        keysTriedCount++;
        console.log(`DeepSeek API key ${currentDeepseekKeyIndex + 1}/${DEEPSEEK_API_KEYS.length} error: ${data.error.message || 'Unknown error'}`);
        continue;
      }
      
      // Extract response content
      let responseContent = null;
      if (data.choices && data.choices[0] && data.choices[0].message) {
        // Standard OpenAI format
        responseContent = data.choices[0].message.content;
      } else if (data.response) {
        // Alternative response format
        responseContent = data.response;
      }
      
      // Check for empty response
      if (!responseContent) {
        // Try next key
        lastError = new Error('Empty response from API');
        getNextDeepseekKey();
        keysTriedCount++;
        console.log(`DeepSeek API key ${currentDeepseekKeyIndex + 1}/${DEEPSEEK_API_KEYS.length} returned empty response`);
        continue;
      }
      
      // Success! Return the response
      return responseContent;
      
    } catch (error) {
      // Try next key
      lastError = error;
      getNextDeepseekKey();
      keysTriedCount++;
      console.log(`DeepSeek API key ${currentDeepseekKeyIndex + 1}/${DEEPSEEK_API_KEYS.length} error: ${error.message}`);
    }
  }
  
  // If we get here, all keys failed
  throw lastError || new Error('All DeepSeek API keys failed');
}

async function callGeminiAPI(messages) {
  // Try all available Gemini keys until one works
  let lastError = null;
  const initialKeyIndex = currentGeminiKeyIndex;
  let keysTriedCount = 0;
  
  // Convert messages to Gemini format
  const geminiContents = [];
  
  // Add system message as a user message with [system] prefix
  const systemMessage = messages.find(msg => msg.role === 'system');
  if (systemMessage) {
    geminiContents.push({
      role: 'user',
      parts: [{ text: `[system] ${systemMessage.content}` }]
    });
  }
  
  // Add the rest of the messages
  for (const msg of messages) {
    if (msg.role !== 'system') {
      geminiContents.push({
        role: msg.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: msg.content }]
      });
    }
  }
  
  while (keysTriedCount < GEMINI_API_KEYS.length) {
    try {
      // Import Gemini API dynamically
      const { GoogleGenerativeAI } = await import('@google/generative-ai');
      
      // Initialize Gemini API
      const genAI = new GoogleGenerativeAI(getCurrentGeminiKey());
      const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });
      
      // Create chat session
      const chat = model.startChat({
        history: geminiContents.slice(0, -1), // All messages except the last one
        generationConfig: {
          maxOutputTokens: 1000,
        },
      });
      
      // Send the last message to get a response
      const lastMessage = geminiContents[geminiContents.length - 1];
      const result = await chat.sendMessage(lastMessage.parts[0].text);
      const response = result.response;
      
      // Check for empty response
      if (!response || !response.text()) {
        // Try next key
        lastError = new Error('Empty response from Gemini API');
        getNextGeminiKey();
        keysTriedCount++;
        console.log(`Gemini API key ${currentGeminiKeyIndex + 1}/${GEMINI_API_KEYS.length} returned empty response`);
        continue;
      }
      
      // Success! Return the response
      return response.text();
      
    } catch (error) {
      // Try next key
      lastError = error;
      getNextGeminiKey();
      keysTriedCount++;
      console.log(`Gemini API key ${currentGeminiKeyIndex + 1}/${GEMINI_API_KEYS.length} error: ${error.message}`);
    }
  }
  
  // If we get here, all keys failed
  throw lastError || new Error('All Gemini API keys failed');
}

// æª¢æ¸¬ç”¨æˆ¶æ˜¯å¦æƒ³è¦ç”Ÿæˆåœ–ç‰‡çš„å‡½æ•¸
async function detectImageGenerationRequest(content) {
  // å®šç¾©å¯èƒ½è¡¨ç¤ºç”¨æˆ¶æƒ³è¦ç”Ÿæˆåœ–ç‰‡çš„é—œéµè©
  const imageGenerationKeywords = [
    'ç•«åœ–', 'ç”Ÿæˆåœ–ç‰‡', 'ç•«ä¸€å¼µ', 'å¹«æˆ‘ç•«', 'å¹«æˆ‘ç”Ÿæˆåœ–ç‰‡', 'å¹«æˆ‘ç”Ÿæˆä¸€å¼µåœ–ç‰‡',
    'generate image', 'create image', 'draw', 'draw me', 'generate a picture',
    'ai ç•«åœ–', 'aiç•«åœ–', 'aiç¹ªåœ–', 'ai ç¹ªåœ–', 'ç•«ä¸€å€‹', 'ç•«å€‹', 'ç”Ÿæˆä¸€å¼µ', 'ç”Ÿä¸€å¼µ',
    'create a picture', 'draw a picture', 'generate an image', 'create an image',
    'å¹«æˆ‘ç•«ä¸€å¼µ', 'å¹«æˆ‘ç•«å€‹', 'å¹«å¿™ç•«', 'å¹«å¿™ç”Ÿæˆåœ–ç‰‡', 'è«‹ç•«', 'è«‹ç”Ÿæˆåœ–ç‰‡'
  ];
  
  /*const imageGenerationKeywords = [
    'qwejasuijasmcjnausf biawe gfisod biUOSDf hnboipawebyu ofguaobusd uhf '
  ];*/
  
  // æª¢æŸ¥å…§å®¹æ˜¯å¦åŒ…å«ä»»ä½•é—œéµè©
  return imageGenerationKeywords.some(keyword => 
    content.toLowerCase().includes(keyword.toLowerCase())
  );
}

// ä½¿ç”¨ Gemini API ç”Ÿæˆåœ–ç‰‡çš„å‡½æ•¸
async function generateImageWithGemini(prompt) {
  // å˜—è©¦æ‰€æœ‰å¯ç”¨çš„ Gemini API å¯†é‘°ï¼Œç›´åˆ°æœ‰ä¸€å€‹æˆåŠŸ
  let lastError = null;
  const initialKeyIndex = currentGeminiKeyIndex;
  let keysTriedCount = 0;
  
  while (keysTriedCount < GEMINI_API_KEYS.length) {
    try {
      // å‹•æ…‹å°å…¥ Gemini API
      const { GoogleGenerativeAI } = await import('@google/generative-ai');
      
      // åˆå§‹åŒ– Gemini API
      const genAI = new GoogleGenerativeAI(getCurrentGeminiKey());
      
      // ç²å–åœ–ç‰‡ç”Ÿæˆæ¨¡å‹
      const model = genAI.getGenerativeModel({ model: "gemini-2.0-flash-preview-image-generation" });
      
      // èª¿ç”¨ Gemini API ç”Ÿæˆåœ–ç‰‡
      const response = await model.generateContent(prompt, { responseModalities: ['IMAGE', 'TEXT'] });
      
      // ç²å–éŸ¿æ‡‰å…§å®¹
      const result = await response.response();
      
      // æª¢æŸ¥éŸ¿æ‡‰
      if (!result || !result.text()) {
        // å˜—è©¦ä¸‹ä¸€å€‹å¯†é‘°
        lastError = new Error('Empty response from Gemini API');
        getNextGeminiKey();
        keysTriedCount++;
        console.log(`Gemini API key ${currentGeminiKeyIndex + 1}/${GEMINI_API_KEYS.length} returned empty response for image generation`);
        continue;
      }
      
      // æå–åœ–ç‰‡æ•¸æ“šå’Œæ–‡æœ¬
      const imageData = result.text();
      const responseText = 'é€™æ˜¯æ ¹æ“šä½ çš„æè¿°ç”Ÿæˆçš„åœ–ç‰‡ï¼š';
      
      // è¿”å›åœ–ç‰‡æè¿°å’ŒéŸ¿æ‡‰æ–‡æœ¬
      return { 
        imageData: imageData, // ä½¿ç”¨ç”Ÿæˆçš„æ–‡æœ¬ä½œç‚ºåœ–ç‰‡æè¿°
        responseText: responseText 
      };
      
    } catch (error) {
      // å˜—è©¦ä¸‹ä¸€å€‹å¯†é‘°
      lastError = error;
      getNextGeminiKey();
      keysTriedCount++;
      console.log(`Gemini API key ${currentGeminiKeyIndex + 1}/${GEMINI_API_KEYS.length} error for image generation: ${error.message}`);
    }
  }
  
  // å¦‚æœæ‰€æœ‰å¯†é‘°éƒ½å¤±æ•—ï¼Œæ‹‹å‡ºéŒ¯èª¤
  throw lastError || new Error('All Gemini API keys failed for image generation');
}

async function callChatGPTAPI(messages) {
  // Try all available ChatGPT keys until one works
  let lastError = null;
  const initialKeyIndex = currentChatGPTKeyIndex;
  let keysTriedCount = 0;
  
  while (keysTriedCount < CHATGPT_API_KEYS.length) {
    try {
      // Import OpenAI API directly - using new SDK version
      const OpenAI = (await import('openai')).default;
      
      // Initialize OpenAI API with new SDK format
      const openai = new OpenAI({ 
        apiKey: getCurrentChatGPTKey(),
        baseURL: 'https://free.v36.cm/v1',
        dangerouslyAllowBrowser: true // Add this option to bypass safety check
      });
      
      // Call ChatGPT API with new SDK format
      const completion = await openai.chat.completions.create({
        model: 'gpt-3.5-turbo-0125',
        messages: messages,
        max_tokens: 500 // Reduced from 1000 to make responses shorter
      });
      
      // Check for empty response
      if (!completion || !completion.choices || !completion.choices[0] || !completion.choices[0].message) {
        // Try next key
        lastError = new Error('Empty response from ChatGPT API');
        getNextChatGPTKey();
        keysTriedCount++;
        console.log(`ChatGPT API key ${currentChatGPTKeyIndex + 1}/${CHATGPT_API_KEYS.length} returned empty response`);
        continue;
      }
      
      // Success! Return the response
      return completion.choices[0].message.content;
      
    } catch (error) {
      // Try next key
      lastError = error;
      getNextChatGPTKey();
      keysTriedCount++;
      console.log(`ChatGPT API key ${currentChatGPTKeyIndex + 1}/${CHATGPT_API_KEYS.length} error: ${error.message}`);
    }
  }
  
  // If we get here, all keys failed
  throw lastError || new Error('All ChatGPT API keys failed');
}

client.on('messageCreate', async (message) => {
  if (message.author.bot) return;

  // Check if the message is in an active channel
  const channelConfig = activeChannels.get(message.channelId);
  if (!channelConfig) return;

  // Show typing indicator immediately
  await message.channel.sendTyping();
  
  // æª¢æŸ¥ç”¨æˆ¶æ˜¯å¦æƒ³è¦ç”Ÿæˆåœ–ç‰‡
  const isImageGenerationRequest = await detectImageGenerationRequest(message.content);
  if (isImageGenerationRequest && GEMINI_API_KEYS.length > 0) {
    try {
      // é¡¯ç¤ºæ­£åœ¨ç”Ÿæˆåœ–ç‰‡çš„æç¤º
      const statusMessage = await message.channel.send('æ­£åœ¨ç”Ÿæˆåœ–ç‰‡ï¼Œè«‹ç¨å€™...');
      
      // ä½¿ç”¨ Gemini ç”Ÿæˆåœ–ç‰‡
      const { imageData, responseText } = await generateImageWithGemini(message.content);
      
      // å°‡åœ–ç‰‡æ•¸æ“šè½‰æ›ç‚º Buffer
      const buffer = Buffer.from(imageData, 'base64');
      
      // å‰µå»ºè‡¨æ™‚æ–‡ä»¶å
      const fileName = `gemini-image-${Date.now()}.png`;
      
      // ç™¼é€åœ–ç‰‡å’ŒéŸ¿æ‡‰æ–‡æœ¬
      await message.channel.send({
        content: responseText || 'é€™æ˜¯æ ¹æ“šä½ çš„æè¿°ç”Ÿæˆçš„åœ–ç‰‡ï¼š',
        files: [{
          attachment: buffer,
          name: fileName
        }]
      });
      
      // åˆªé™¤ç‹€æ…‹æ¶ˆæ¯
      await statusMessage.delete().catch(console.error);
      
      console.log(`Generated image for ${message.author.username} with prompt: ${message.content}`);
      return; // åœ–ç‰‡ç”Ÿæˆè«‹æ±‚å·²è™•ç†ï¼Œä¸éœ€è¦é€²ä¸€æ­¥è™•ç†
    } catch (error) {
       console.error('Error generating image:', error);
       await message.channel.send('æŠ±æ­‰ï¼Œç”Ÿæˆåœ–ç‰‡æ™‚å‡ºç¾éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚');
       // ç¹¼çºŒè™•ç†æ¶ˆæ¯ï¼Œè®“ AI å›æ‡‰
    }
  }
  
  // æª¢æŸ¥æ¶ˆæ¯æ˜¯å¦åŒ…å«åœ–ç‰‡é™„ä»¶
  let imageAttachmentInfo = "";
  if (message.attachments.size > 0) {
    const imageAttachments = message.attachments.filter(attachment => {
      const fileExtension = attachment.name.split('.').pop().toLowerCase();
      return ['png', 'jpg', 'jpeg', 'gif', 'webp'].includes(fileExtension);
    });
    
    if (imageAttachments.size > 0) {
      // å°‡åœ–ç‰‡ä¿¡æ¯æ·»åŠ åˆ°æ¶ˆæ¯å…§å®¹ä¸­
      imageAttachmentInfo = "\n\n[IMAGE SHARED BY " + message.author.username + ": " + 
        imageAttachments.map(attachment => `${attachment.url}`).join(", ") + "]\n\n";
      
      console.log(`Detected image attachment from ${message.author.username}: \n${imageAttachmentInfo}`);
    }
  }

  // Check for YouTube URLs or search queries
  const youtubeUrlRegex = /(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:watch\?v=|embed\/|v\/)|youtu\.be\/)([\w-]+)/i;
  const youtubeUrlMatch = message.content.match(youtubeUrlRegex);

  if (youtubeUrlMatch && youtubeUrlMatch[1]) {
    const videoId = youtubeUrlMatch[1];
    try {
      const youtube = google.youtube({
        version: 'v3',
        auth: process.env.YOUTUBE_API_KEY
      });
      const response = await youtube.videos.list({
        part: 'snippet,statistics',
        id: videoId
      });
      if (response.data.items && response.data.items.length > 0) {
        const video = response.data.items[0];
        const embed = {
          color: 0xFF0000,
          title: video.snippet.title,
          url: `https://www.youtube.com/watch?v=${videoId}`,
          author: {
            name: video.snippet.channelTitle,
            url: `https://www.youtube.com/channel/${video.snippet.channelId}`
          },
          description: video.snippet.description.substring(0, 200) + (video.snippet.description.length > 200 ? '...' : ''),
          thumbnail: { url: video.snippet.thumbnails.medium.url },
          fields: [
            { name: 'è§€çœ‹æ¬¡æ•¸', value: video.statistics.viewCount ? parseInt(video.statistics.viewCount).toLocaleString() : 'N/A', inline: true },
            { name: 'å–œæ­¡äººæ•¸', value: video.statistics.likeCount ? parseInt(video.statistics.likeCount).toLocaleString() : 'N/A', inline: true },
          ],
          timestamp: new Date(video.snippet.publishedAt),
          footer: { text: 'YouTube' }
        };
        await message.channel.send({ embeds: [embed] });
        
        // ä¿®æ”¹ï¼šä¸è¦ç›´æ¥è¿”å›ï¼Œè€Œæ˜¯å°‡å½±ç‰‡ä¿¡æ¯æ·»åŠ åˆ°æ¶ˆæ¯ä¸­ï¼Œè®“AIè™•ç†
        const videoInfo = {
          title: video.snippet.title,
          channel: video.snippet.channelTitle,
          description: video.snippet.description,
          url: `https://www.youtube.com/watch?v=${videoId}`,
          views: video.statistics.viewCount ? parseInt(video.statistics.viewCount).toLocaleString() : 'N/A',
          likes: video.statistics.likeCount ? parseInt(video.statistics.likeCount).toLocaleString() : 'N/A',
          publishDate: new Date(video.snippet.publishedAt).toLocaleDateString()
        };
        
        // å°‡åŸå§‹æ¶ˆæ¯å…§å®¹ä¿®æ”¹ç‚ºåŒ…å«å½±ç‰‡ä¿¡æ¯å’Œç™¼é€è€…è¨Šæ¯ï¼Œä½¿ç”¨æ›´æ˜ç¢ºçš„æ ¼å¼
        message.content = `${message.content}\n\n[YOUTUBE VIDEO SHARED BY ${message.author.username}:\nTitle: "${videoInfo.title}"\nChannel: "${videoInfo.channel}"\nDescription: "${videoInfo.description.substring(0, 300)}${videoInfo.description.length > 300 ? '...' : ''}"\nViews: ${videoInfo.views}\nLikes: ${videoInfo.likes}\nPublished: ${videoInfo.publishDate}]\n\n[Message sent by: ${message.author.username}]`;
        
        // ç¹¼çºŒè™•ç†æ¶ˆæ¯ï¼Œä¸è¦è¿”å›
      }
    } catch (error) {
      console.error('Error fetching YouTube video by URL:', error);
      // Continue to AI response if fetching fails
    }
  }

  // Keywords to detect YouTube search intent
  const youtubeSearchKeywords = ['youtube', 'å½±ç‰‡', 'yt', 'æ‰¾å½±ç‰‡', 'æœå½±ç‰‡'];
  const containsYoutubeKeyword = youtubeSearchKeywords.some(keyword => message.content.toLowerCase().includes(keyword));

  if (containsYoutubeKeyword && process.env.YOUTUBE_API_KEY) {
    let searchQuery = message.content;
    // Attempt to extract a more specific query if possible
    // This is a simple heuristic, can be improved
    youtubeSearchKeywords.forEach(keyword => {
      searchQuery = searchQuery.replace(new RegExp(keyword, 'gi'), '').trim();
    });
    if (searchQuery.length > 2) { // Avoid overly broad or empty searches
      try {
        const youtube = google.youtube({
          version: 'v3',
          auth: process.env.YOUTUBE_API_KEY
        });
        const searchResponse = await youtube.search.list({
          part: 'snippet',
          q: searchQuery,
          maxResults: 3, // Show fewer results for natural language queries
          type: 'video'
        });

        if (searchResponse.data.items && searchResponse.data.items.length > 0) {
          const videos = searchResponse.data.items.map(item => ({
            title: item.snippet.title,
            url: `https://www.youtube.com/watch?v=${item.id.videoId}`,
            channelTitle: item.snippet.channelTitle
          }));
          const embed = {
            color: 0xFF0000,
            title: `æˆ‘æ‰¾åˆ°äº†é€™äº› YouTube å½±ç‰‡çµ¦ä½ åƒè€ƒçœ‹çœ‹ï¼š${searchQuery}`,
            description: videos.map((video, index) => `${index + 1}. [${video.title}](${video.url}) - ${video.channelTitle}`).join('\n'),
            thumbnail: { url: 'https://www.youtube.com/s/desktop/28b0985e/img/favicon_144x144.png' }
          };
          await message.channel.send({ embeds: [embed] });
          
          // ä¿®æ”¹ï¼šä¸è¦ç›´æ¥è¿”å›ï¼Œè€Œæ˜¯å°‡æœç´¢çµæœæ·»åŠ åˆ°æ¶ˆæ¯ä¸­ï¼Œè®“AIè™•ç†
          const videoInfoText = videos.map((video, index) => 
            `Video ${index + 1}: "${video.title}" by ${video.channelTitle}`
          ).join('\n');
          
          // å°‡åŸå§‹æ¶ˆæ¯å…§å®¹ä¿®æ”¹ç‚ºåŒ…å«æœç´¢çµæœå’Œç™¼é€è€…è¨Šæ¯
          message.content = `${message.content}\n\n[YouTube Search Results for "${searchQuery}":\n${videoInfoText}]\n\n[Message sent by: ${message.author.username}]`;
          
          // ç¹¼çºŒè™•ç†æ¶ˆæ¯ï¼Œä¸è¦è¿”å›
        }
      } catch (error) {
        console.error('Error searching YouTube via natural language:', error);
        // Continue to AI response if search fails
      }
    }
  }
  
// Check if the message is a reply to another message

let replyContext = "";
let isReply = false;

if (message.reference && message.reference.messageId) {
  try {
    // å–å¾—è¢«å›è¦†çš„è¨Šæ¯
    const repliedMessage = await message.channel.messages.fetch(message.reference.messageId);

    if (repliedMessage) {
      isReply = true;
      const repliedAuthor = repliedMessage.author.bot ? "Setsuna" : repliedMessage.author.username;
      replyContext = `[å›è¦† ${repliedAuthor} çš„è¨Šæ¯: "${repliedMessage.content}"] `;

      console.log(`Detected reply to message: ${repliedMessage.content}`);
    }

  } catch (error) {
    console.error('Error fetching replied message:', error);
  }
}

// æŠ“å–æœ€è¿‘çš„ 50 å‰‡è¨Šæ¯ä½œç‚ºå°è©±æ­·å²
const messages = await message.channel.messages.fetch({ limit: 50 });
const messageHistory = Array.from(messages.values())
  .reverse()
  .map(msg => ({
    role: msg.author.bot ? 'assistant' : 'user',
    content: msg.content,
    author: msg.author.username
  }));

// å¦‚æœæ˜¯å›è¦†ï¼Œå°‡åŸè¨Šæ¯å…§å®¹åŠ ä¸Šä¸Šä¸‹æ–‡èªªæ˜
if (isReply) {
  for (let i = 0; i < messageHistory.length; i++) {
    if (
      messageHistory[i].role === 'user' &&
      messageHistory[i].content === message.content
    ) {
      messageHistory[i].content = replyContext + message.content;
      break;
    }
  }
}
  
  // å¦‚æœæœ‰åœ–ç‰‡é™„ä»¶ï¼Œå°‡åœ–ç‰‡ä¿¡æ¯æ·»åŠ åˆ°æ¶ˆæ¯å…§å®¹ä¸­
  if (imageAttachmentInfo) {
    message.content = message.content + imageAttachmentInfo;
    
    // æ›´æ–°æ¶ˆæ¯æ­·å²ä¸­çš„æœ€å¾Œä¸€æ¢æ¶ˆæ¯
    for (let i = 0; i < messageHistory.length; i++) {
      if (
        messageHistory[i].role === 'user' &&
        messageHistory[i].author === message.author.username
      ) {
        messageHistory[i].content = messageHistory[i].content + imageAttachmentInfo;
        break;
      }
    }
  }
  
  // Update channel's message history
  channelConfig.messageHistory = messageHistory;
  
  // Process with selected API
  try {
    // Add personality prompt as system message
    const formattedMessages = [
      { role: 'system', content: setsunaPersonality },
      ...messageHistory.map(msg => ({
        role: msg.role,
        content: msg.content
      }))
    ];
    
    // Get channel's preferred model or use default
    const preferredModel = channelModelPreferences.get(message.channelId) || defaultModel;
    
    // Variables to track response
    let response = null;
    let modelUsed = '';
    let fallbackUsed = false;
    
    // Try preferred model first
    switch (preferredModel) {
      case 'deepseek':
        if (DEEPSEEK_API_KEYS.length > 0) {
          try {
            response = await callDeepseekAPI(formattedMessages);
            modelUsed = 'DeepSeek';
          } catch (error) {
            console.log('DeepSeek API error:', error.message);
            // Will fall back to other models
          }
        }
        break;
        
      case 'gemini':
        if (GEMINI_API_KEYS.length > 0) {
          try {
            response = await callGeminiAPI(formattedMessages);
            modelUsed = 'Gemini';
          } catch (error) {
            console.log('Gemini API error:', error.message);
            // Will fall back to other models
          }
        }
        break;
        
      case 'chatgpt':
        if (CHATGPT_API_KEYS.length > 0) {
          try {
            response = await callChatGPTAPI(formattedMessages);
            modelUsed = 'ChatGPT';
          } catch (error) {
            console.log('ChatGPT API error:', error.message);
            // Will fall back to other models
          }
        }
        break;
        
      case 'groq':
        if (GROQ_API_KEYS.length > 0) {
          try {
            response = await callGroqAPI(formattedMessages, message.channelId);
            const groqModel = channelGroqModelPreferences.get(message.channelId) || defaultGroqModel;
            modelUsed = `Groq (${groqModel})`;
          } catch (error) {
            console.log('Groq API error:', error.message);
            // Will fall back to other models
          }
        }
        break;
        
      case 'together':
        if (TOGETHER_API_KEYS.length > 0) {
          try {
            response = await callTogetherAPI(formattedMessages);
            modelUsed = 'Together AI';
          } catch (error) {
            console.log('Together API error:', error.message);
            // Will fall back to other models
          }
        }
        break;
    }
    
    // If preferred model failed, try other models as fallback
    if (!response) {
      fallbackUsed = true;
      
      // Try Groq API if not already tried and keys are available
      if (!response && preferredModel !== 'groq' && GROQ_API_KEYS.length > 0) {
        try {
          response = await callGroqAPI(formattedMessages, message.channelId);
          const groqModel = channelGroqModelPreferences.get(message.channelId) || defaultGroqModel;
          modelUsed = `Groq (${groqModel})`;
        } catch (error) {
          console.log('Groq API fallback error:', error.message);
        }
      }
      
      // Try Gemini API if not already tried and keys are available
      if (!response && preferredModel !== 'gemini' && GEMINI_API_KEYS.length > 0) {
        try {
          response = await callGeminiAPI(formattedMessages);
          modelUsed = 'Gemini';
        } catch (error) {
          console.log('Gemini API fallback error:', error.message);
        }
      }
      
      // Try ChatGPT API if not already tried and keys are available
      if (!response && preferredModel !== 'chatgpt' && CHATGPT_API_KEYS.length > 0) {
        try {
          response = await callChatGPTAPI(formattedMessages);
          modelUsed = 'ChatGPT';
        } catch (error) {
          console.log('ChatGPT API fallback error:', error.message);
        }
      }
      
      // Try Together API if not already tried and keys are available
      if (!response && preferredModel !== 'together' && TOGETHER_API_KEYS.length > 0) {
        try {
          response = await callTogetherAPI(formattedMessages);
          modelUsed = 'Together AI';
        } catch (error) {
          console.log('Together API fallback error:', error.message);
        }
      }
      
      // Try DeepSeek API if not already tried and keys are available (last resort)
      if (!response && preferredModel !== 'deepseek' && DEEPSEEK_API_KEYS.length > 0) {
        try {
          response = await callDeepseekAPI(formattedMessages);
          modelUsed = 'DeepSeek';
        } catch (error) {
          console.log('DeepSeek API fallback error:', error.message);
        }
      }
    }
    
    // If all models failed or returned empty response
    if (!response) {
      throw new Error('All available models failed to generate a response');
    }
    
    // Refresh typing indicator
    await message.channel.sendTyping();
    
    // Send the response
    await message.channel.send(response);
    if (fallbackUsed) {
      console.log(`Response sent using ${modelUsed} model (fallback from ${preferredModel})`);
    } else {
      console.log(`Response sent using ${modelUsed} model`);
    }
    
  } catch (error) {
    console.error('Error generating response:', error);
    await message.channel.send('Sorry, I glitched out for a sec. Hit me up again later?');
  }
  },
),



client.on('error', (error) => {
  console.error('Discord client error:', error);
});

// Handle process termination gracefully
process.on('SIGINT', () => {
  console.log('Bot is shutting down...');
  client.destroy();
  process.exit(0);
});

process.on('unhandledRejection', (error) => {
  console.error('Unhandled promise rejection:', error);
});

// Connect to Discord
console.log('Connecting to Discord...');
client.login(DISCORD_TOKEN).catch(error => {
  console.error('Failed to login to Discord:', error);
  process.exit(1);
});